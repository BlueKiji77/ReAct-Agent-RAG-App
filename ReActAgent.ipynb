{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3788f7fb-10ac-4e2f-ba71-27d1674002cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4f9612-16d6-4be8-88db-9c1765e2bc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment from /teamspace/studios/this_studio/react_agent/config.env\n",
      "Successfully downloaded: /teamspace/studios/this_studio/react_agent/src/utils/src/data/10k/uber_2021.pdf\n",
      "Successfully downloaded: /teamspace/studios/this_studio/react_agent/src/utils/src/data/10k/lyft_2021.pdf\n",
      "Loaded environment from /teamspace/studios/this_studio/react_agent/config.env\n",
      "Loading embeding model\n",
      "Found existing indices, loading from disk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_deploy.message_queues.simple - Launching message queue server at 127.0.0.1:8001\n",
      "INFO:     Started server process [12910]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:llama_deploy.control_plane.server - Launching control plane server at 127.0.0.1:8000\n",
      "INFO:     Started server process [12910]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8001 (Press CTRL+C to quit)\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
      "INFO:llama_deploy.message_queues.simple - Consumer ControlPlaneServer-34d5da49-4c3d-489c-b99e-c4c37ac1e112: control_plane has been registered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:50906 - \"POST /register_consumer HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from src import Config\n",
    "from llama_deploy import deploy_workflow, deploy_core, WorkflowServiceConfig, ControlPlaneConfig, SimpleMessageQueueConfig\n",
    "from src import ReActAgent\n",
    "from src.utils import download_10k_reports\n",
    "from src.utils import load_hf_model\n",
    "from src.utils import initialize_groq_model\n",
    "from src.ingest import setup_indices\n",
    "from llama_index.core import PromptTemplate\n",
    "from src.query_engine import HybridRetriver, StuffedContextQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.postprocessor.colbert_rerank import ColbertRerank\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.indices.vector_store.retrievers import VectorIndexRetriever\n",
    "\n",
    "async def main(local_model=False):\n",
    "    download_10k_reports()\n",
    "    config = Config()\n",
    "    if local_model:\n",
    "        hf_models = load_hf_model(config.hf_tiny_model, config.hf_embed_model)\n",
    "    else:\n",
    "        hf_models = load_hf_model(config.hf_tiny_model, config.hf_embed_model, embed_model_only=True)\n",
    "        groq_llama_8b = initialize_groq_model(\"llama3-8b-8192\")\n",
    "        groq_llama_70b = initialize_groq_model(\"llama3-70b-8192\")\n",
    "    \n",
    "    indices = setup_indices(hf_models['embed_model'])\n",
    "    lyft_index = indices['lyft_index']\n",
    "    uber_index = indices['uber_index']\n",
    "\n",
    "    # lyft_retriever = HybridRetriver(index=lyft_index, vector_similarity_top_k=20, bm25_similarity_top_k=20, fusion_similarity_top_k=20, llm=groq_llama_8b)\n",
    "    # uber_retriever = HybridRetriver(index=uber_index, vector_similarity_top_k=20, bm25_similarity_top_k=20, fusion_similarity_top_k=20, llm=groq_llama_8b)\n",
    "    \n",
    "    lyft_retriever = VectorIndexRetriever(index=lyft_index, similarity_top_k=20)\n",
    "    uber_retriever = VectorIndexRetriever(index=uber_index, similarity_top_k=20)\n",
    "    \n",
    "    colbert_reranker = ColbertRerank(\n",
    "        top_n=10,\n",
    "        model=\"colbert-ir/colbertv2.0\",\n",
    "        tokenizer=\"colbert-ir/colbertv2.0\",\n",
    "        keep_retrieval_score=True,\n",
    "    )\n",
    "    \n",
    "    postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7),\n",
    "                    colbert_reranker]                       \n",
    "        \n",
    "    qa_prompt = PromptTemplate(\n",
    "        \"\"\"\\\n",
    "    Context information is below.\n",
    "    ---------------------\n",
    "    {context_str}\n",
    "    ---------------------\n",
    "    Given the context information and not prior knowledge, answer the query.\n",
    "    Query: {query_str}\n",
    "    Answer: \\\n",
    "    \"\"\"\n",
    "    )\n",
    "    \n",
    "    lyft_query_engine = StuffedContextQueryEngine(\n",
    "                        retriever=lyft_retriever, \n",
    "                        qa_prompt=qa_prompt,\n",
    "                        llm=groq_llama_8b,\n",
    "                        node_postprocessors=postprocessors\n",
    "                        )\n",
    "    \n",
    "    uber_query_engine = StuffedContextQueryEngine(\n",
    "                        retriever=uber_retriever, \n",
    "                        qa_prompt=qa_prompt,\n",
    "                        llm=groq_llama_8b,\n",
    "                        node_postprocessors=postprocessors\n",
    "                        )\n",
    "    \n",
    "    query_engine_tools = [\n",
    "        QueryEngineTool(\n",
    "            query_engine=lyft_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"lyft_10k\",\n",
    "                description=\"Provides information about Lyft financials for year 2021\",\n",
    "            ),\n",
    "        ),\n",
    "        QueryEngineTool(\n",
    "            query_engine=uber_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"uber_10k\",\n",
    "                description=\"Provides information about Uber financials for year 2021\",\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "    agent = ReActAgent(\n",
    "            llm=groq_llama_8b,\n",
    "            tools=query_engine_tools,\n",
    "            timeout=120,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "    # # Define the Gradio interface\n",
    "    # def chat_interface(user_input):\n",
    "    #     # Start the workflow with the user input\n",
    "    #     result = agent.run(user_inpu)\n",
    "        \n",
    "    #     # Extract the response from the result\n",
    "    #     response = result.result[\"response\"]\n",
    "    #     return response\n",
    "\n",
    "    # def chat_with_agent(user_input, state):\n",
    "    #     # Append the user's message to the conversation history\n",
    "    #     state.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    #     print(f\"Userinput is: {user_input}\")\n",
    "    #     print(f\"State is: {state}\")\n",
    "        \n",
    "    #     output = agent.run(user_input)\n",
    "    #     response = output.result[\"response\"]\n",
    "    #     sources = output.result[\"sources\"]\n",
    "    #     reasoning = output.result[\"reasoning\"]\n",
    "    \n",
    "    #     # Append the agent's response to the conversation history\n",
    "    #     state.append({\"role\": \"assistant\", \"content\": response})\n",
    "        \n",
    "    #     print(f\"Response is: {response}\")\n",
    "    #     print(f\"State after run is: {state}\")\n",
    "        \n",
    "    #     return state, response #, \\n\\n\".join(sources), \"\\n\\n\".join(reasoning)\n",
    "\n",
    "    # def reset_state():\n",
    "    #     return []\n",
    "\n",
    "    \n",
    "    # demo = gr.Blocks()\n",
    "    # with demo:\n",
    "    #     gr.Markdown(\"# React Agent Chat\")\n",
    "        \n",
    "    #     with gr.Row():\n",
    "    #         chatbot = gr.Chatbot([], elem_id=\"chatbot\", type=\"messages\")\n",
    "    #         state = gr.State([])\n",
    "    \n",
    "    #     user_input = gr.Textbox(\n",
    "    #         show_label=False,\n",
    "    #         placeholder=\"Enter your message...\",\n",
    "    #         lines=2,\n",
    "    #         max_lines=4,\n",
    "    #     )\n",
    "    \n",
    "    #     # Submit action on Enter key press\n",
    "    #     user_input.submit(chat_with_agent, [user_input, state], [state, chatbot]).then(\n",
    "    #         lambda: gr.update(value=''),  # Clear the textbox after submission\n",
    "    #         None,\n",
    "    #         [user_input]\n",
    "    #     )\n",
    "    \n",
    "    #     submit_button = gr.Button(\"Submit\")\n",
    "    #     submit_button.click(chat_with_agent, [user_input, state], [state, chatbot]).then(\n",
    "    #         lambda: gr.update(value=''),  # Clear the textbox after clicking submit\n",
    "    #         None,\n",
    "    #         [user_input]\n",
    "    #     )\n",
    "    \n",
    "    #     reset_button = gr.Button(\"Reset\")\n",
    "    #     reset_button.click(reset_state, None, state)\n",
    "    \n",
    "    # demo.launch(share=True)\n",
    "\n",
    "    # # Create the Gradio interface\n",
    "    # print('Building interface ...')\n",
    "    # iface = gr.Interface(\n",
    "    #     fn=chat_interface,\n",
    "    #     inputs=\"text\",\n",
    "    #     outputs=\"text\",\n",
    "    #     title=\"ReAct Agent Chat\",\n",
    "    #     description=\"Interact with the ReAct Agent\",\n",
    "    # )\n",
    "    # # Launch the interface\n",
    "    # iface.launch(share=True)\n",
    "    \n",
    "    await deploy_core(\n",
    "        control_plane_config=ControlPlaneConfig(),\n",
    "        message_queue_config=SimpleMessageQueueConfig(),\n",
    "    )\n",
    "    \n",
    "    await deploy_workflow(\n",
    "        ReActAgent(\n",
    "            llm=groq_llama_8b,\n",
    "            tools=query_engine_tools,\n",
    "            timeout=120,\n",
    "            verbose=False\n",
    "        ),\n",
    "        # 10.192.12.249 is the IP address ofmy cloud provider\n",
    "        WorkflowServiceConfig(host=\"10.192.12.249\", port=8000, service_name=\"my_workflow\"),\n",
    "        ControlPlaneConfig(host=\"10.192.12.249\", port=8000),\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9364ffa8-4212-4001-bce0-2787c00c04f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
